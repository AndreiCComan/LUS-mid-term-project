{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DISI](../resources/DISI.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the json library for reading the configuration file\n",
    "import json\n",
    "\n",
    "# import the subprocess library for making command line calls\n",
    "import subprocess\n",
    "\n",
    "# import the math library for doing math operations :)\n",
    "import math\n",
    "\n",
    "# import the operator which will be used to sort dictionaries\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the configuration file\n",
    "CONFIGURATION_FILE = \"config.json\"\n",
    "\n",
    "# open the configuration file\n",
    "json_configuration_file = open(CONFIGURATION_FILE, \"r\", encoding=\"utf8\")\n",
    "\n",
    "# load the configuration file\n",
    "CONFIGURATION = json.load(json_configuration_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the configuration file parameters\n",
    "TRAIN_FILE = CONFIGURATION[\"train_file\"]\n",
    "TRAIN_FEATS_FILE = CONFIGURATION[\"train_feats_file\"]\n",
    "TEST_FILE = CONFIGURATION[\"test_file\"]\n",
    "TEST_FEATS_FILE = CONFIGURATION[\"test_feats_file\"]\n",
    "ADDITIONAL_FEATURE = CONFIGURATION[\"additional_feature\"]\n",
    "IMPROVEMENT = CONFIGURATION[\"improvement\"]\n",
    "SMOOTHING = CONFIGURATION[\"smoothing\"]\n",
    "HANDLE_UNK = CONFIGURATION[\"handle_unk\"]\n",
    "NGRAM_SIZE = int(CONFIGURATION[\"ngram_size\"])\n",
    "BACKOFF = CONFIGURATION[\"backoff\"]\n",
    "BINS = int(CONFIGURATION[\"bins\"])\n",
    "WITTEN_BELL_K = float(CONFIGURATION[\"witten_bell_k\"])\n",
    "DISCOUNT_D = float(CONFIGURATION[\"discount_D\"])\n",
    "\n",
    "# this variable is used to generate the ouput directory name\n",
    "OUTPUT_DIR = CONFIGURATION[\"output_dir\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameter assumes the right values\n",
    "assert(ADDITIONAL_FEATURE == \"none\" or\\\n",
    "       ADDITIONAL_FEATURE == \"tokenpos\" or\\\n",
    "       ADDITIONAL_FEATURE == \"lemmapos\" or\\\n",
    "       ADDITIONAL_FEATURE == \"lemma\"),\\\n",
    "\"> ADDITIONAL_FEATURE value must be either <none>, <tokenpos>, <lemmapos> or <lemma>, provided value is <{0}>\".format(ADDITIONAL_FEATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameter assumes the right values\n",
    "assert(IMPROVEMENT == \"none\" or\\\n",
    "       IMPROVEMENT == \"wise\" or\\\n",
    "       IMPROVEMENT == \"naive\"),\\\n",
    "\"> IMPROVEMENTS value must be either <none>, <wise> or <naive>, provided value is <{0}>\".format(IMPROVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameter assumes the right values\n",
    "assert(SMOOTHING == \"witten_bell\" or\\\n",
    "       SMOOTHING == \"katz\" or\\\n",
    "       SMOOTHING == \"kneser_ney\" or\\\n",
    "       SMOOTHING == \"absolute\" or\\\n",
    "       SMOOTHING == \"presmoothed\" or\\\n",
    "       SMOOTHING == \"unsmoothed\"),\\\n",
    "\"> SMOOTHING value must be either <witten_bell>, <katz>, <kneser_ney>, <absolute>, <presmoothed> or <unsmoothed> provided value is <{0}>\".format(SMOOTHING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameter assumes the right values\n",
    "assert(BACKOFF == \"true\" or\\\n",
    "       BACKOFF == \"false\"),\\\n",
    "\"> BACKOFF value must be either <true> or <false>, provided value is <{0}>\".format(BACKOFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameter assumes the right values\n",
    "assert(NGRAM_SIZE >= 1),\\\n",
    "\"> NGRAM_SIZE must be greater than or equal to 1, provided value is <{0}>\".format(NGRAM_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the parameter assumes the right values\n",
    "assert(HANDLE_UNK == \"uniform\" or\\\n",
    "       HANDLE_UNK.startswith(\"cut_off_\")),\\\n",
    "\"> HANDLE_UNK value must be either <uniform> or <cut_off_#>, provided value is <{0}>\".format(HANDLE_UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the output directory name\n",
    "if len(OUTPUT_DIR) == 0:\n",
    "    OUTPUT_DIR += \"AF_{0}_IM_{1}_SM_{2}_NS_{3}_HU_{4}\"\\\n",
    "              .format(ADDITIONAL_FEATURE, IMPROVEMENT, SMOOTHING, NGRAM_SIZE, HANDLE_UNK)\n",
    "else:\n",
    "    OUTPUT_DIR += \"/AF_{0}_IM_{1}_SM_{2}_NS_{3}_HU_{4}\"\\\n",
    "              .format(ADDITIONAL_FEATURE, IMPROVEMENT, SMOOTHING, NGRAM_SIZE, HANDLE_UNK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input:\n",
    "#     * input file from which we take the concept\n",
    "#     * features file from which we take the POS-tag or the lemma\n",
    "#     * parameter that decides which feature (POS-tag or lemma) to take into account and the way it will be used\n",
    "# it returns the path of the generated file\n",
    "def apply_additional_feature(input_file_path, features_file_path, additional_feature):\n",
    "    \n",
    "    print(\"> applying {0} additional feature ...\".format(additional_feature))\n",
    "    \n",
    "    # define the path for the output file\n",
    "    output_file_path = \"{0}_{1}.txt\".format(input_file_path.split(\".\")[0], additional_feature)\n",
    "    \n",
    "    # open the input file (read mode)\n",
    "    input_file = open(input_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the features file (read mode)\n",
    "    features_file = open(features_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the output file (write mode)\n",
    "    output_file = open(output_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # list for the input file lines\n",
    "    input_file_lines = []\n",
    "    \n",
    "    # list for the features file lines\n",
    "    features_file_lines = []\n",
    "    \n",
    "    # iterate over each line within the input file\n",
    "    for line in input_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # append the line in the input file lines list\n",
    "        input_file_lines.append(line)\n",
    "    \n",
    "    # iterate over each line within the features file\n",
    "    for line in features_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # append the line in the features file lines list\n",
    "        features_file_lines.append(line)\n",
    "    \n",
    "    # make sure the input file and features file number of lines match\n",
    "    assert(len(input_file_lines) == len(features_file_lines)),\\\n",
    "    \"> length of the two files should match\"\n",
    "    \n",
    "    # for each line in the output file\n",
    "    for line_index in range(len(input_file_lines)):\n",
    "        \n",
    "        # take the line from the input file\n",
    "        input_file_line = input_file_lines[line_index]\n",
    "        \n",
    "        # take the line from the features file\n",
    "        features_file_line = features_file_lines[line_index]\n",
    "        \n",
    "        # if end of sentence in the input file (blank line)\n",
    "        if len(input_file_line) == 0:\n",
    "            \n",
    "            # makes sure is the same for the features file\n",
    "            assert(len(features_file_line) == 0),\\\n",
    "            \"> line should be empty\"\n",
    "            \n",
    "            # then write the blank line to the output file\n",
    "            output_file.write(\"\\n\")\n",
    "            \n",
    "            # go to the next line\n",
    "            continue\n",
    "        \n",
    "        # if not end of sentences, split the input file line\n",
    "        input_file_line_split = input_file_line.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(input_file_line_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(input_file_line_split))\n",
    "        \n",
    "        # split the features file line\n",
    "        features_file_line_split = features_file_line.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(features_file_line_split) == 3),\\\n",
    "        \"> expected 3 tab separated values, found {0} instead\".format(len(features_file_line_split))\n",
    "        \n",
    "        # take the token from the input file line\n",
    "        input_file_token = input_file_line_split[0]\n",
    "        \n",
    "        # take the concept from the input file line\n",
    "        input_file_concept = input_file_line_split[1]\n",
    "        \n",
    "        # take the token from the features file line\n",
    "        features_file_token = features_file_line_split[0]\n",
    "        \n",
    "        # take the POS-tag from the features file line\n",
    "        features_file_pos = features_file_line_split[1]\n",
    "        \n",
    "        # take the lemma from the features file line\n",
    "        features_file_lemma = features_file_line_split[2]\n",
    "        \n",
    "        # make sure the input token and features token match\n",
    "        assert(input_file_token == features_file_token),\\\n",
    "        \"> token should match between the two file\"\n",
    "        \n",
    "        # check which feature should be used and generate the token for the output file\n",
    "        if additional_feature == \"tokenpos\":\n",
    "            \n",
    "            # create the tokenpos token\n",
    "            tokenpos = \"{0}_{1}\".format(input_file_token, features_file_pos)\n",
    "            \n",
    "            # write the tokenpos token and the input file concept to the output file\n",
    "            output_file.write(\"{0}\\t{1}\\n\".format(tokenpos, input_file_concept))\n",
    "            \n",
    "        elif additional_feature == \"lemmapos\":\n",
    "            \n",
    "            # create the lemmapos token\n",
    "            lemmapos = \"{0}_{1}\".format(features_file_lemma, features_file_pos)\n",
    "            \n",
    "            # write the lemmapos token and the input file concept to the output file\n",
    "            output_file.write(\"{0}\\t{1}\\n\".format(lemmapos, input_file_concept))\n",
    "            \n",
    "        else: # additional_feature == \"lemma\":\n",
    "            \n",
    "            # write the lemma token and the input file concept to the output file\n",
    "            output_file.write(\"{0}\\t{1}\\n\".format(features_file_lemma, input_file_concept))\n",
    "    \n",
    "    # close the input file\n",
    "    input_file.close()\n",
    "    \n",
    "    # close the features file\n",
    "    features_file.close()\n",
    "    \n",
    "    # close the output file\n",
    "    output_file.close()\n",
    "    \n",
    "    # return the output file path\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input a file and generates a modified version of it by following this example:\n",
    "#\n",
    "# the following sentence\n",
    "#\n",
    "#       show\tO\n",
    "#       credits\tO\n",
    "#       for\tO\n",
    "#       the\tB-movie.name\n",
    "#       godfather\tI-movie.name\n",
    "#\n",
    "# becomes\n",
    "#\n",
    "#       show\tO\n",
    "#       credits\tO\n",
    "#       for\t_for\n",
    "#       the\tB-movie.name\n",
    "#       godfather\tI-movie.name\n",
    "#\n",
    "# it returns the path of the generated file\n",
    "def apply_wise_improvement(input_file_path):\n",
    "    \n",
    "    print(\"> applying wise improvement ...\")\n",
    "    \n",
    "    # define the path for the output file\n",
    "    output_file_path = \"{0}_wise.txt\".format(input_file_path.split(\".\")[0])\n",
    "    \n",
    "    # open the output file (write mode)\n",
    "    output_file = open(output_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the input file (read mode)\n",
    "    input_file = open(input_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # list for the input file lines\n",
    "    input_file_lines = []\n",
    "    \n",
    "    # iterate over each line within the input file\n",
    "    for line in input_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # append the line in the input file lines list\n",
    "        input_file_lines.append(line)\n",
    "    \n",
    "    # iterate over each line in the input file lines list\n",
    "    for current_line_index in range(len(input_file_lines)):\n",
    "        \n",
    "        # get the next line index\n",
    "        next_line_index = current_line_index + 1\n",
    "        \n",
    "        # retrieve the next line\n",
    "        current_line = input_file_lines[current_line_index]\n",
    "        \n",
    "        # check for lower boundary\n",
    "        # check for upper boundary\n",
    "        # check for blank line (end of sentence)\n",
    "        if (current_line_index == 0) or (next_line_index > len(input_file_lines)) or (len(current_line) == 0):\n",
    "                \n",
    "                # write blank line to the output file\n",
    "                output_file.write(\"{0}\\n\".format(current_line))\n",
    "                \n",
    "                # go to the next line\n",
    "                continue\n",
    "        \n",
    "        # if not end of sentences, split the current line\n",
    "        current_line_split = current_line.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(current_line_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(current_line_split))\n",
    "        \n",
    "        # take the token from the current line\n",
    "        current_line_token = current_line_split[0]\n",
    "        \n",
    "        # take the concept from the current line\n",
    "        current_line_concept= current_line_split[1]\n",
    "        \n",
    "        # if the current line concept is an O\n",
    "        if current_line_concept == \"O\":\n",
    "            \n",
    "            # retrieve the next line\n",
    "            next_line = input_file_lines[next_line_index]\n",
    "            \n",
    "            # if next line is a blank line\n",
    "            if (len(next_line) == 0):\n",
    "                \n",
    "                # write the current line to the output file\n",
    "                output_file.write(\"{0}\\n\".format(current_line))\n",
    "                \n",
    "            else: # if not a blank line\n",
    "                \n",
    "                # split the next line\n",
    "                next_line_split = next_line.split(\"\\t\")\n",
    "                \n",
    "                # make sure the split produces the expected number of arguments\n",
    "                assert(len(next_line_split) == 2),\\\n",
    "                \"> expected 2 tab separated values, found {0} instead\".format(len(next_line_split))\n",
    "                \n",
    "                # take the concept from the next line\n",
    "                next_line_concept = next_line_split[1]\n",
    "                \n",
    "                # check if next line concept is an O\n",
    "                if (next_line_concept == \"O\"):\n",
    "                    \n",
    "                    # if it is then write the current line to the output file\n",
    "                    output_file.write(\"{0}\\n\".format(current_line))\n",
    "                    \n",
    "                else: # if next line concept not an O\n",
    "                    \n",
    "                    # write the current line token and as token, and the current line token preceded by an underscore as concept\n",
    "                    output_file.write(\"{0}\\t_{0}\\n\".format(current_line_token))\n",
    "                    \n",
    "        else: # if current line concept is not an O\n",
    "            \n",
    "            # write the current line to the output file\n",
    "            output_file.write(\"{0}\\n\".format(current_line))\n",
    "    \n",
    "    # close the input file\n",
    "    input_file.close()\n",
    "    \n",
    "    # close the output file\n",
    "    output_file.close()\n",
    "    \n",
    "    # return the output file path\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input a file and generates a modified version of it by following this example:\n",
    "#\n",
    "# the following sentence\n",
    "#\n",
    "#       show\tO\n",
    "#       credits\tO\n",
    "#       for\tO\n",
    "#       the\tB-movie.name\n",
    "#       godfather\tI-movie.name\n",
    "#\n",
    "# becomes\n",
    "#\n",
    "#       show\t_show\n",
    "#       credits\t_credits\n",
    "#       for\t_for\n",
    "#       the\tB-movie.name\n",
    "#       godfather\tI-movie.name\n",
    "#\n",
    "# it returns the path of the generated file\n",
    "def apply_naive_improvement(input_file_path):\n",
    "    \n",
    "    # define the path for the output file\n",
    "    output_file_path = \"{0}_naive.txt\".format(input_file_path.split(\".\")[0])\n",
    "    \n",
    "    # open the output file (write mode)\n",
    "    output_file = open(output_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the input file (read mode)\n",
    "    input_file = open(input_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # iterate over each line within the input file\n",
    "    for line in input_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # if end of sentence (blank line)\n",
    "        if len(line) == 0:\n",
    "            \n",
    "            # write black line to the output file\n",
    "            output_file.write(\"\\n\")\n",
    "            \n",
    "            # go to the next line\n",
    "            continue\n",
    "        \n",
    "        # if not blank line, split it\n",
    "        line_split = line.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(line_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(line_split))\n",
    "        \n",
    "        # take the token from the line\n",
    "        token = line_split[0]\n",
    "        \n",
    "        # take the concept from the line\n",
    "        concept = line_split[1]\n",
    "        \n",
    "        # check if concept is an O\n",
    "        if concept == \"O\":\n",
    "            \n",
    "            # write token as token, and the token preceded by an underscore as concept to the output file\n",
    "            output_file.write(\"{0}\\t_{0}\\n\".format(token))\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # write the line to the output file\n",
    "            output_file.write(\"{0}\\t{1}\\n\".format(token, concept))\n",
    "    \n",
    "    # close input file\n",
    "    input_file.close()\n",
    "    \n",
    "    # close output file\n",
    "    output_file.close()\n",
    "    \n",
    "    # return the output file path\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes an input a file and generates different files, such as:\n",
    "#     * token counts file\n",
    "#     * concept counts file\n",
    "#     * token-concept counts file\n",
    "#     * token-concept probabilities file\n",
    "# it returns some useful dictionaries\n",
    "def generate_useful_files(train_file_path, output_dir):\n",
    "    \n",
    "    print(\"> generating useful files ...\")\n",
    "    \n",
    "    # define the path for the token counts file\n",
    "    token_counts_file_path = \"{0}/token.counts\".format(output_dir)\n",
    "    \n",
    "    # define the path for the concept counts file\n",
    "    concept_counts_file_path = \"{0}/concept.counts\".format(output_dir)\n",
    "    \n",
    "    # define the path for the token-concept counts file\n",
    "    token_concept_counts_file_path = \"{0}/token_concept.counts\".format(output_dir)\n",
    "    \n",
    "    # define the path for the token-concept probabilities file\n",
    "    token_concept_probs_file_path = \"{0}/token_concept.probs\".format(output_dir)\n",
    "    \n",
    "    # open the train file (read mode)\n",
    "    train_file = open(train_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the token counts file (write mode)\n",
    "    token_counts_file = open(token_counts_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the concept counts file (write mode)\n",
    "    concept_counts_file = open(concept_counts_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the token-cocept counts file (write mode)\n",
    "    token_concept_counts_file = open(token_concept_counts_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the token-concept probabilities file (write mode)\n",
    "    token_concept_probs_file = open(token_concept_probs_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # declare the token counts dictionary\n",
    "    token_counts_dictionary = {}\n",
    "    \n",
    "    # declare the concept counts dictionary\n",
    "    concept_counts_dictionary = {}\n",
    "    \n",
    "    # declare the token-concept counts dictionary\n",
    "    token_concept_counts_dictionary = {}\n",
    "    \n",
    "    # declare the token-concept probabilities dictionary\n",
    "    token_concept_probs_dictionary = {}\n",
    "    \n",
    "    # iterate over each line in the train file\n",
    "    for line in train_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # if end of sentence (blank line)\n",
    "        if len(line) == 0:\n",
    "            \n",
    "            # go to the next line\n",
    "            continue\n",
    "        \n",
    "        # if not end of sentence, split the line\n",
    "        line_split = line.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(line_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(line_split))\n",
    "        \n",
    "        # take the token from the line\n",
    "        token = line_split[0]\n",
    "        \n",
    "        # take the concept from the line\n",
    "        concept = line_split[1]\n",
    "        \n",
    "        # create the token-concept pair\n",
    "        token_concept = \"{0}\\t{1}\".format(token, concept)\n",
    "        \n",
    "        # check if token was already inserted within the token counts dictionary\n",
    "        if token not in token_counts_dictionary:\n",
    "        \n",
    "            # if not then add it to the dictionary\n",
    "            token_counts_dictionary[token] = 1\n",
    "        \n",
    "        else: # if it was already inserted\n",
    "            \n",
    "            # increment the count by 1\n",
    "            token_counts_dictionary[token] += 1\n",
    "        \n",
    "        # check if concept was already inserted within the concept counts dictionary\n",
    "        if concept not in concept_counts_dictionary:\n",
    "            \n",
    "            # if not then add it to the dictionary\n",
    "            concept_counts_dictionary[concept] = 1\n",
    "        \n",
    "        else: # if it was already inserted\n",
    "            \n",
    "            # increment the count by 1\n",
    "            concept_counts_dictionary[concept] += 1\n",
    "        \n",
    "        # check if token-concept was already inserted within the token-concept counts dictionary\n",
    "        if token_concept not in token_concept_counts_dictionary:\n",
    "            \n",
    "            # if not then add it to the dictionary\n",
    "            token_concept_counts_dictionary[token_concept] = 1\n",
    "        \n",
    "        else: # if it was already inserted\n",
    "            \n",
    "            # increment the count by 1\n",
    "            token_concept_counts_dictionary[token_concept] += 1\n",
    "    \n",
    "    # compute token-concept probabilities and populate the token-concept proabilities dictionary\n",
    "    for token_concept in token_concept_counts_dictionary:\n",
    "        \n",
    "        # split the token concept pair\n",
    "        token_concept_split = token_concept.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(token_concept_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(token_concept_split))\n",
    "        \n",
    "        # take the concept from the token-concept pair\n",
    "        concept = token_concept_split[1]\n",
    "        \n",
    "        # compute the token-concept probability\n",
    "        token_concept_prob = -math.log(float(token_concept_counts_dictionary[token_concept]) / float(concept_counts_dictionary[concept]))\n",
    "        \n",
    "        # add the token-concept probability to the dictionary\n",
    "        token_concept_probs_dictionary[token_concept] = token_concept_prob\n",
    "    \n",
    "    # sort (by value) the token counts dictionary\n",
    "    sorted_token_counts_list = sorted(token_counts_dictionary.items(), key = itemgetter(1), reverse = True)\n",
    "    \n",
    "    # sort (by value) the concept counts dictionary\n",
    "    sorted_concept_counts_list = sorted(concept_counts_dictionary.items(), key = itemgetter(1), reverse = True)\n",
    "    \n",
    "    # sort (by value) the token-concept counts dictionary\n",
    "    sorted_token_concept_counts_list = sorted(token_concept_counts_dictionary.items(), key = itemgetter(1), reverse = True)\n",
    "    \n",
    "    # sort (by value) the token-concept probabilities dictionary\n",
    "    sorted_token_concept_probs_list = sorted(token_concept_probs_dictionary.items(), key = itemgetter(1), reverse = True)\n",
    "    \n",
    "    # write the sorted token counts dictionary to file\n",
    "    for token, token_count in sorted_token_counts_list:\n",
    "        token_counts_file.write(\"{0}\\t{1}\\n\".format(token, token_count))\n",
    "    \n",
    "    # write the sorted concept counts dictionary to file\n",
    "    for concept, concept_count in sorted_concept_counts_list:\n",
    "        concept_counts_file.write(\"{0}\\t{1}\\n\".format(concept, concept_count))\n",
    "        \n",
    "    # write the sorted token-concept counts dictionary to file\n",
    "    for token_concept, token_concept_count in sorted_token_concept_counts_list:\n",
    "        token_concept_counts_file.write(\"{0}\\t{1}\\n\".format(token_concept, token_concept_count))\n",
    "\n",
    "    # write the sorted token-concept probabilities dictionary to file\n",
    "    for token_concept, token_concept_prob in sorted_token_concept_probs_list:\n",
    "        token_concept_probs_file.write(\"{0}\\t{1}\\n\".format(token_concept, token_concept_prob))\n",
    "    \n",
    "    # close train file\n",
    "    train_file.close()\n",
    "    \n",
    "    # close the token counts file\n",
    "    token_counts_file.close()\n",
    "    \n",
    "    # close the concept counts file\n",
    "    concept_counts_file.close()\n",
    "    \n",
    "    # close the token-concept counts file\n",
    "    token_concept_counts_file.close()\n",
    "    \n",
    "    # close the token-concept probabilities file\n",
    "    token_concept_probs_file.close()    \n",
    "    \n",
    "    # return dictionaries\n",
    "    return (token_counts_dictionary,\\\n",
    "            concept_counts_dictionary,\\\n",
    "            token_concept_counts_dictionary,\\\n",
    "            token_concept_probs_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input a file and generates a lexicon and transducer in the passed output directory path\n",
    "# handle unknown-concept with a uniform probability approach\n",
    "def generate_lexicon_and_transducer_uniform(train_file_path, output_dir):\n",
    "    \n",
    "    print(\"> generating lexicon and transducer uniform ...\")\n",
    "    \n",
    "    # define the path for the lexicon file\n",
    "    lexicon_file_path = \"{0}/lexicon.lex\".format(output_dir)\n",
    "    \n",
    "    # define the path for the transducer file\n",
    "    transducer_file_path = \"{0}/transducer_uniform.txt\".format(output_dir)\n",
    "    \n",
    "    # open the lexicon file (write mode)\n",
    "    lexicon_file = open(lexicon_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the transducer file (write mode)\n",
    "    transducer_file = open(transducer_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # get dictionaries\n",
    "    token_counts_dictionary,\\\n",
    "    concept_counts_dictionary,\\\n",
    "    token_concept_counts_dictionary,\\\n",
    "    token_concept_probs_dictionary = generate_useful_files(train_file_path, output_dir)\n",
    "    \n",
    "    # GENERATE LEXICON\n",
    "    \n",
    "    # lexeme list\n",
    "    lexeme_list = []\n",
    "    \n",
    "    # lexeme index\n",
    "    lexeme_index = 0\n",
    "    \n",
    "    # add to the lexeme list the first lexeme\n",
    "    lexeme_list.append(\"<epsilon>\")\n",
    "    \n",
    "    # write the first lexeme to the lexicon file\n",
    "    lexicon_file.write(\"<epsilon>\\t{0}\\n\".format(lexeme_index))\n",
    "    \n",
    "    # increment the lexeme index\n",
    "    lexeme_index += 1\n",
    "    \n",
    "    # iterate over each token\n",
    "    for token, token_count in token_counts_dictionary.items():\n",
    "        \n",
    "        # check if token was already inserted in the lexeme list\n",
    "        if token not in lexeme_list:\n",
    "            \n",
    "            # if not then add it to the lexeme list\n",
    "            lexeme_list.append(token)\n",
    "            \n",
    "            # write it to the lexicon file\n",
    "            lexicon_file.write(\"{0}\\t{1}\\n\".format(token, lexeme_index))\n",
    "            \n",
    "            # increment the lexeme index\n",
    "            lexeme_index +=1\n",
    "    \n",
    "    # iterate over each concept\n",
    "    for concept, concept_count in concept_counts_dictionary.items():\n",
    "        \n",
    "        # check if concept was alreary inserted in the lexeme list\n",
    "        if concept not in lexeme_list:\n",
    "            \n",
    "            # if not then add it ot the lexeme list\n",
    "            lexeme_list.append(concept)\n",
    "            \n",
    "            # write it to the lexicon file\n",
    "            lexicon_file.write(\"{0}\\t{1}\\n\".format(concept, lexeme_index))\n",
    "            \n",
    "            # increment the lexeme index\n",
    "            lexeme_index +=1\n",
    "    \n",
    "    # add to the lexeme list the last lexeme\n",
    "    lexeme_list.append(\"<unk>\")\n",
    "\n",
    "    # write the last lexeme to the lexicon file\n",
    "    lexicon_file.write(\"<unk>\\t{0}\\n\".format(lexeme_index))\n",
    "    \n",
    "    # increment the lexeme index\n",
    "    lexeme_index += 1\n",
    "           \n",
    "    # GENERATE TRANSDUCER\n",
    "    \n",
    "    # devine the uniform probability of the unkown-concept pair\n",
    "    unk_concept_prob = -math.log(1/len(concept_counts_dictionary))\n",
    "    \n",
    "    # iterate over each token-concept probability pair\n",
    "    for token_concept, token_concept_prob in token_concept_probs_dictionary.items():\n",
    "        \n",
    "        # split the token-concept pair\n",
    "        token_concept_split = token_concept.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(token_concept_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(token_concept_split))\n",
    "        \n",
    "        # take the token from the token-concept pair\n",
    "        token = token_concept_split[0]\n",
    "        \n",
    "        # take the concept from the token-concept pair\n",
    "        concept = token_concept_split[1]\n",
    "        \n",
    "        # write to the transducer file the token concept transition with the corresponding probability\n",
    "        transducer_file.write(\"0\\t0\\t{0}\\t{1}\\t{2}\\n\".format(token, concept, token_concept_prob))\n",
    "     \n",
    "    # iterate over each concept\n",
    "    for concept, concept_count in concept_counts_dictionary.items():\n",
    "        \n",
    "        # write to the transducer file the unkown-concept transition with the corresponding probability\n",
    "        transducer_file.write(\"0\\t0\\t{0}\\t{1}\\t{2}\\n\".format(\"<unk>\", concept, unk_concept_prob))\n",
    "    \n",
    "    # write the final state to the transducer file\n",
    "    transducer_file.write(\"0\")\n",
    "    \n",
    "    # close the lexicon file\n",
    "    lexicon_file.close()\n",
    "    \n",
    "    # close the transducer file\n",
    "    transducer_file.close()\n",
    "    \n",
    "    # return the lexicon file path and transducer file path\n",
    "    return (lexicon_file_path, transducer_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes as input a file and generates a lexicon and transducer in the passed output directory path\n",
    "# handle unkown-concept with a cut-off approach\n",
    "#     1) take the token-concept counts < threshold\n",
    "#     2) group by concept\n",
    "#     3) accumulate the counts\n",
    "#     4) divive each concept accumulated counts by the total token-concept counts\n",
    "#     5) assign this value as unknown-concept probability\n",
    "def generate_lexicon_and_transducer_cut_off(train_file_path, cut_off_value, output_dir):\n",
    "    \n",
    "    print(\"> generating lexicon and transducer cut-off-{0} ...\".format(cut_off_value))\n",
    "    \n",
    "    # define the path for the lexicon file\n",
    "    lexicon_file_path = \"{0}/lexicon.lex\".format(output_dir)\n",
    "    \n",
    "    # define the path for the transducer file\n",
    "    transducer_file_path = \"{0}/transducer_cut_off_{1}.txt\".format(output_dir, cut_off_value)\n",
    "    \n",
    "    # open the lexicon file (write mode)\n",
    "    lexicon_file = open(lexicon_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the transducer file (write mode)\n",
    "    transducer_file = open(transducer_file_path, \"w\", encoding=\"utf8\")\n",
    "\n",
    "    # get dictionaries\n",
    "    token_counts_dictionary,\\\n",
    "    concept_counts_dictionary,\\\n",
    "    token_concept_counts_dictionary,\\\n",
    "    token_concept_probs_dictionary = generate_useful_files(train_file_path, output_dir)\n",
    "    \n",
    "    # GENERATE TRANSDUCER\n",
    "    \n",
    "    # token-concept counts over threshold dictionary\n",
    "    extracted_token_concept_counts_dictionary = {}\n",
    "    \n",
    "    # concept accumulated counts dictionary\n",
    "    concepts_under_threshold_counts_dictionary = {}\n",
    "    \n",
    "    # total token-concept counts\n",
    "    total_token_concept_counts = 0\n",
    "    \n",
    "    # iterate over each token-concept count\n",
    "    for token_concept, token_concept_count in token_concept_counts_dictionary.items():\n",
    "        \n",
    "        # split the token-concept pair\n",
    "        token_concept_split = token_concept.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(token_concept_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(token_concept_split))\n",
    "        \n",
    "        # take the token from the token-concept pair\n",
    "        token = token_concept_split[0]\n",
    "        \n",
    "        # take the concept from the token-concept pair\n",
    "        concept = token_concept_split[1]\n",
    "        \n",
    "        # add the token-concept count to the total token-concept counts\n",
    "        total_token_concept_counts += token_concept_count\n",
    "        \n",
    "        # check if the token-concept count is under the threshold\n",
    "        if token_concept_count <= cut_off_value:\n",
    "                \n",
    "                # if it is then check if the concept was already inserted in the concepts under threshold dictionary\n",
    "                if concept not in concepts_under_threshold_counts_dictionary:\n",
    "                    \n",
    "                    # if not then add it with the corresponding token-concept count\n",
    "                    concepts_under_threshold_counts_dictionary[concept] = token_concept_count\n",
    "                \n",
    "                else: # if it was already inserted\n",
    "                    \n",
    "                    # accumulate the token-concept count for this concept\n",
    "                    concepts_under_threshold_counts_dictionary[concept] += token_concept_count\n",
    "                    \n",
    "        else: # if token-concept count is over the threshold\n",
    "            \n",
    "            # add the token-concept pair with the corresponding count to the token-concept over the threshold dictionary\n",
    "            extracted_token_concept_counts_dictionary[token_concept] = token_concept_count\n",
    "    \n",
    "    # iterate over each token-concept pair within the token-concept over the threshold dictionary\n",
    "    for token_concept, token_concept_count in extracted_token_concept_counts_dictionary.items():\n",
    "        \n",
    "        # split the token-concept pair\n",
    "        token_concept_split = token_concept.split(\"\\t\")\n",
    "\n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(token_concept_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(token_concept_split))\n",
    "\n",
    "        # take the token from the token-concept pair\n",
    "        token = token_concept_split[0]\n",
    "        \n",
    "        # take the concept from the token-concept pair\n",
    "        concept = token_concept_split[1]\n",
    "        \n",
    "        # compute the probability for the token-concept pair\n",
    "        token_concept_prob = -math.log(float(extracted_token_concept_counts_dictionary[token_concept]) / float(concept_counts_dictionary[concept]))\n",
    "        \n",
    "        # write to the transducer file the token concept transition with the corresponding probability\n",
    "        transducer_file.write(\"0\\t0\\t{0}\\t{1}\\t{2}\\n\".format(token, concept, token_concept_prob))\n",
    "    \n",
    "    # iterate over each concept under threshold\n",
    "    for concept_under_threshold, concept_under_threshold_count in concepts_under_threshold_counts_dictionary.items():\n",
    "        \n",
    "        # define the unkown word\n",
    "        unk = \"<unk>\"\n",
    "        \n",
    "        # take the concept under threshold\n",
    "        concept = concept_under_threshold\n",
    "        \n",
    "        # compute the unkown-concept probability\n",
    "        unk_concept_prob = -math.log(concept_under_threshold_count / total_token_concept_counts)\n",
    "        \n",
    "        # write to the transducer file the unkown-concept transition with the corresponding probability\n",
    "        transducer_file.write(\"0\\t0\\t{0}\\t{1}\\t{2}\\n\".format(unk, concept, unk_concept_prob))\n",
    "    \n",
    "    # write the final state to the transducer file\n",
    "    transducer_file.write(\"0\")\n",
    "    \n",
    "    # GENERATE LEXICON\n",
    "    \n",
    "    # lexeme list\n",
    "    lexeme_list = []\n",
    "    \n",
    "    # lexeme index\n",
    "    lexeme_index = 0\n",
    "    \n",
    "    # add to the lexeme list the first lexeme\n",
    "    lexeme_list.append(\"<epsilon>\")\n",
    "    \n",
    "    # write the first lexeme to the lexicon file\n",
    "    lexicon_file.write(\"<epsilon>\\t{0}\\n\".format(lexeme_index))\n",
    "    \n",
    "    # increment the lexeme index\n",
    "    lexeme_index += 1\n",
    "    \n",
    "    # iterate over each token-concept pair\n",
    "    for token_concept, token_concept_count in extracted_token_concept_counts_dictionary.items():\n",
    "        \n",
    "        # split the token-concept pair\n",
    "        token_concept_split = token_concept.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(token_concept_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(token_concept_split))\n",
    "        \n",
    "        # take the token from the token-concept pair\n",
    "        token = token_concept_split[0]\n",
    "        \n",
    "        # take the concept from the token-concept pair\n",
    "        concept = token_concept_split[1]\n",
    "        \n",
    "        # check if token was already inserted in the lexeme list\n",
    "        if token not in lexeme_list:\n",
    "            \n",
    "            # if not then add it\n",
    "            lexeme_list.append(token)\n",
    "            \n",
    "            # write it to the lexicon file\n",
    "            lexicon_file.write(\"{0}\\t{1}\\n\".format(token, lexeme_index))\n",
    "            \n",
    "            # increment the lexeme index\n",
    "            lexeme_index += 1\n",
    "            \n",
    "        # check if concept was already inserted in the lexeme list    \n",
    "        if concept not in lexeme_list:\n",
    "            \n",
    "            # if not then add it\n",
    "            lexeme_list.append(concept)\n",
    "            \n",
    "            # write it to the lexicon file\n",
    "            lexicon_file.write(\"{0}\\t{1}\\n\".format(concept, lexeme_index))\n",
    "            \n",
    "            # increment the lexeme index\n",
    "            lexeme_index += 1\n",
    "    \n",
    "    # iterate over each concept under threshold\n",
    "    for concept_under_threshold, concept_under_threshold_count in concepts_under_threshold_counts_dictionary.items():\n",
    "        \n",
    "        # check if concept under threshold was already inserted in the lexeme list\n",
    "        if concept_under_threshold not in lexeme_list:\n",
    "            \n",
    "            # if not then add it\n",
    "            lexeme_list.append(concept_under_threshold)\n",
    "            \n",
    "            # write it to the lexicon file\n",
    "            lexicon_file.write(\"{0}\\t{1}\\n\".format(concept_under_threshold, lexeme_index))\n",
    "            \n",
    "            # increment the lexeme index\n",
    "            lexeme_index += 1\n",
    "    \n",
    "    # add to the lexeme list the last lexeme\n",
    "    lexeme_list.append(\"<unk>\")\n",
    "    \n",
    "    # write the last lexeme to the lexicon file\n",
    "    lexicon_file.write(\"<unk>\\t{0}\\n\".format(lexeme_index))\n",
    "    \n",
    "    # increment the lexeme index\n",
    "    lexeme_index += 1\n",
    "    \n",
    "    # close the lexicon file\n",
    "    lexicon_file.close()\n",
    "    \n",
    "    # close the transducer file\n",
    "    transducer_file.close()\n",
    "    \n",
    "    # return the lexicon file path and transducer file path\n",
    "    return (lexicon_file_path, transducer_file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper function that decides which specific function should be called\n",
    "def generate_lexicon_and_transducer(train_file_path, handle_unk, output_dir):\n",
    "    \n",
    "    # handle unknown-concept with a uniform probability approach\n",
    "    if handle_unk == \"uniform\":\n",
    "        \n",
    "        # call the specific function\n",
    "        return generate_lexicon_and_transducer_uniform(train_file_path, output_dir)\n",
    "    \n",
    "    else: # handle unknown-concept with a cut-off approach\n",
    "        \n",
    "        # retrieve the cut-off threshold\n",
    "        cut_off_value = int(handle_unk.replace(\"cut_off_\", \"\"))\n",
    "        \n",
    "        # call the specific function\n",
    "        return generate_lexicon_and_transducer_cut_off(train_file_path, cut_off_value, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function compiles a transducer in the passed output directory,\n",
    "#      by taking a lexicon and transducer (text format) as input\n",
    "def compile_transducer(lexicon_file_path, transducer_file_path, output_dir):\n",
    "    \n",
    "    print(\"> compiling transducer ...\")\n",
    "    \n",
    "    # define the trasnducer path\n",
    "    transducer_fst_file_path = \"{0}/transducer.fst\".format(output_dir)\n",
    "    \n",
    "    # call the command line\n",
    "    assert(subprocess.call(\"fstcompile --isymbols={0} --osymbols={0} {1} | fstarcsort > {2}\"\n",
    "                           .format(lexicon_file_path,\\\n",
    "                                   transducer_file_path,\\\n",
    "                                   transducer_fst_file_path), shell = True) == 0)\n",
    "    \n",
    "    # return the compiled transducer file path\n",
    "    return transducer_fst_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function extracts the concepts from sentences and outputs them \n",
    "#      in a file specified in the passed output directory\n",
    "def create_concept_sentences(train_file_path, output_dir):\n",
    "    \n",
    "    print(\"> creating concept sentences ...\")\n",
    "    \n",
    "    # define the concept sentences file path\n",
    "    concept_sentences_file_path = \"{0}/concept_sentences.txt\".format(output_dir)\n",
    "    \n",
    "    # open the train file (read mode)\n",
    "    train_file = open(train_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the concept sentences file (write mode)\n",
    "    concept_sentences_file = open(concept_sentences_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # list of concepts for a single sentence\n",
    "    sentence_concept_list = []\n",
    "    \n",
    "    # iterate over each line in the train file\n",
    "    for line in train_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # check for end of sentence (blank line)\n",
    "        if len(line) == 0:\n",
    "            \n",
    "            # ceate the sentence from the list of concepts\n",
    "            sentence = \" \".join(sentence_concept_list)\n",
    "            \n",
    "            # write the sentence to the concept sentences file\n",
    "            concept_sentences_file.write(\"{0}\\n\".format(sentence))\n",
    "            \n",
    "            # empty the list of concepts for a single sentence\n",
    "            sentence_concept_list = []\n",
    "            \n",
    "            # go to the next line\n",
    "            continue\n",
    "        \n",
    "        # if not end of sentence, split the line\n",
    "        line_split = line.split(\"\\t\")\n",
    "        \n",
    "        # make sure the split produces the expected number of arguments\n",
    "        assert(len(line_split) == 2),\\\n",
    "        \"> expected 2 tab separated values, found {0} instead\".format(len(line_split))\n",
    "        \n",
    "        # get the concept from the line\n",
    "        concept = line_split[1]\n",
    "        \n",
    "        # insert the concept in the list of concepts for a single sentence\n",
    "        sentence_concept_list.append(concept)\n",
    "        \n",
    "    # close the train file\n",
    "    train_file.close()\n",
    "    \n",
    "    # close the concept sentences file\n",
    "    concept_sentences_file.close()\n",
    "    \n",
    "    # return the concept sentences file path\n",
    "    return concept_sentences_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates the language model\n",
    "def create_language_model(lexicon_file_path, concept_sentences_file_path, ngram_size, smoothing, backoff, bins, witten_bell_k, discount_D, output_dir):\n",
    "    \n",
    "    print(\"> creating language model ...\")\n",
    "    \n",
    "    # define the language model far format file path\n",
    "    language_model_far_file_path = \"{0}/language_model.far\".format(output_dir)\n",
    "    \n",
    "    # define the language model cnt format file path\n",
    "    language_model_cnt_file_path = \"{0}/language_model.cnt\".format(output_dir)\n",
    "    \n",
    "    # define the language model lm format file path\n",
    "    language_model_lm_file_path = \"{0}/language_model.lm\".format(output_dir)\n",
    "    \n",
    "    # generate the language model far format file\n",
    "    assert(subprocess.call(\"farcompilestrings --symbols={0} --unknown_symbol='<unk>' {1} > {2}\"\n",
    "           .format(lexicon_file_path, concept_sentences_file_path, language_model_far_file_path), shell = True) == 0)\n",
    "    \n",
    "    # generate the language model cnt format file\n",
    "    assert(subprocess.call(\"ngramcount --order={0} --require_symbols=false {1} > {2}\"\n",
    "           .format(ngram_size, language_model_far_file_path, language_model_cnt_file_path), shell = True) == 0)\n",
    "    \n",
    "    # generate the language model lm format file\n",
    "    assert(subprocess.call(\"ngrammake --method={0} --backoff={1} --bins={2} --witten_bell_k={3} --discount_D={4} {5} > {6}\"\n",
    "           .format(smoothing, backoff, bins, witten_bell_k, discount_D, language_model_cnt_file_path, language_model_lm_file_path), shell = True) == 0)\n",
    "    \n",
    "    # return the language model lm format file path\n",
    "    return language_model_lm_file_path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function compiles (creates a trasnducer of) a string given as input\n",
    "def compile_string(sentence, lexicon_file_path, output_dir):\n",
    "        \n",
    "    # define the compiled string file path\n",
    "    result_compiled_string_file_path = \"{0}/compiled_string.fst\".format(output_dir)\n",
    "    \n",
    "    # compile the string\n",
    "    assert(subprocess.call(\"echo \\\"{0}\\\" | farcompilestrings --symbols={1} --unknown_symbol='<unk>' --generate_keys=1 --keep_symbols | farextract --filename_suffix='.fst'\"\\\n",
    "           .format(sentence, lexicon_file_path), shell = True) == 0),\\\n",
    "    \"> unable to compile string\"\n",
    "    \n",
    "    # move the compiled string to the corresponding directory\n",
    "    assert(subprocess.call(\"mv 1.fst {0}\".format(result_compiled_string_file_path), shell = True) == 0),\\\n",
    "    \"> unable to move file\"\n",
    "    \n",
    "    # return the compiled string file path\n",
    "    return result_compiled_string_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function does the composition between:\n",
    "#     * compiled_string\n",
    "#     * transducer\n",
    "#     * language model\n",
    "def execute_composition(compiled_string_file_path, transducer_fst_file_path, language_model_lm_file_path, lexicon_file_path, output_dir):\n",
    "        \n",
    "    # define the composition file path\n",
    "    result_composition_file_path = \"{0}/composition_result.txt\".format(output_dir)\n",
    "    \n",
    "    # call the composition command\n",
    "    assert(subprocess.call(\"fstcompose {0} {1} | fstcompose - {2} | fstrmepsilon | fstshortestpath | fsttopsort | fstprint --isymbols={3} --osymbols={3} > {4}\"\\\n",
    "                           .format(compiled_string_file_path,\\\n",
    "                                   transducer_fst_file_path,\\\n",
    "                                   language_model_lm_file_path,\\\n",
    "                                   lexicon_file_path,\\\n",
    "                                   result_composition_file_path), shell = True) == 0),\\\n",
    "    \"> unable to compose\"\n",
    "    \n",
    "    # return the composition file path\n",
    "    return result_composition_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function tags a file\n",
    "def tag_file(test_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, output_dir):\n",
    "    \n",
    "    print(\"> tagging file ...\")\n",
    "    \n",
    "    # define the tagged file path\n",
    "    result_tagged_file_path = \"{0}/result_tagged_file.txt\".format(output_dir)\n",
    "    \n",
    "    # open the tagged file (write mode)\n",
    "    result_tagged_file = open(result_tagged_file_path, \"w\", encoding=\"utf8\")\n",
    "    \n",
    "    # open the test file\n",
    "    test_file = open(test_file_path, \"r\", encoding=\"utf8\")\n",
    "    \n",
    "    # number of tagged senteces count\n",
    "    sentences_tagged_count = 0\n",
    "    \n",
    "    # list of tokens per sentence in the test file\n",
    "    list_of_test_tokens = []\n",
    "    \n",
    "    # list of concepts per sentence in the test file\n",
    "    list_of_test_concepts = []\n",
    "    \n",
    "    # iterate over each line in the test file\n",
    "    for line in test_file:\n",
    "        \n",
    "        # get rid of the newline character\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        \n",
    "        # check if end of sentece (blank line)\n",
    "        if len(line) == 0:\n",
    "            \n",
    "            # make sure the lenght of test tokens and concepts match\n",
    "            assert(len(list_of_test_tokens) == len(list_of_test_concepts)),\\\n",
    "            \"> tokens list and concepts list length should match\"\n",
    "            \n",
    "            # create the sentence from tokens\n",
    "            sentence = \" \".join(list_of_test_tokens)\n",
    "            \n",
    "            # compile the sentences\n",
    "            compiled_string_file_path = compile_string(sentence, lexicon_file_path, output_dir)\n",
    "            \n",
    "            # compose the sentence with the transducer and the language model\n",
    "            composition_result_file_path = execute_composition(compiled_string_file_path,\\\n",
    "                                                               transducer_fst_file_path,\\\n",
    "                                                               language_model_lm_file_path,\\\n",
    "                                                               lexicon_file_path,\\\n",
    "                                                               output_dir)\n",
    "            \n",
    "            # open the composition file\n",
    "            composition_result_file = open(composition_result_file_path, \"r\", encoding=\"utf8\")\n",
    "            \n",
    "            # list of tokens per sentence in the composition file\n",
    "            list_of_result_tokens = []\n",
    "            \n",
    "            # list of concepts per sentence in the composition file\n",
    "            list_of_result_concepts = []\n",
    "            \n",
    "            # iterate over each line in the composition file\n",
    "            for line in composition_result_file:\n",
    "                \n",
    "                # get rid of the newline character\n",
    "                line = line.replace(\"\\n\", \"\")\n",
    "                \n",
    "                # split the line\n",
    "                line_split = line.split(\"\\t\")\n",
    "                \n",
    "                # make sure the split produces the expected number of arguments\n",
    "                assert((len(line_split) == 5 or len(line_split) == 2)),\\\n",
    "                \"> expected 5 OR 2 tab separated values, found {0} instead\".format(len(line_split))\n",
    "                \n",
    "                # check if the read line is not a final state\n",
    "                if (len(line_split) == 5): # not final state\n",
    "                    \n",
    "                    # take the token from the line\n",
    "                    result_token = line_split[2]\n",
    "                    \n",
    "                    # take the concept from the line\n",
    "                    result_concept = line_split[3]\n",
    "                    \n",
    "                    # insert the token in the list of tokens per sentence in the composition file\n",
    "                    list_of_result_tokens.append(result_token)\n",
    "                    \n",
    "                    # insert the concept in the list of concepts per sentence in the composition file\n",
    "                    list_of_result_concepts.append(result_concept)\n",
    "            \n",
    "            # close the composition file\n",
    "            composition_result_file.close()\n",
    "            \n",
    "            # make sure the number of test and composition tokens match\n",
    "            assert(len(list_of_test_tokens) == len(list_of_result_tokens)),\\\n",
    "            \"> length of test tokens does not match with the length of result tokens\"\n",
    "            \n",
    "            # make sure the number of test and composition concepts match\n",
    "            assert(len(list_of_test_concepts) == len(list_of_result_concepts)),\\\n",
    "            \"> length of test concepts does not match with the length of result concepts\"\n",
    "            \n",
    "            # write to the tagged file\n",
    "            for index in range(len(list_of_test_tokens)):\n",
    "                \n",
    "                result_tagged_file.write(\"{0} {1} {2}\\n\".format(list_of_test_tokens[index],\\\n",
    "                                                                list_of_test_concepts[index],\\\n",
    "                                                                list_of_result_concepts[index]))\n",
    "            # add end of sentence to the tagged file\n",
    "            result_tagged_file.write(\"\\n\")\n",
    "            \n",
    "            # empty the list of tokens in the test file\n",
    "            list_of_test_tokens = []\n",
    "            \n",
    "            # empty the list of concepts in the test file\n",
    "            list_of_test_concepts = []\n",
    "            \n",
    "            # increment the number of tagged sentences\n",
    "            sentences_tagged_count += 1\n",
    "            \n",
    "            # print the current number of analyzed sentences\n",
    "            print(\"# of analyzed sentences: {0}\".format(sentences_tagged_count), end=\"\\r\")\n",
    "            \n",
    "        else: # if not end of sentence\n",
    "            \n",
    "            # split the line\n",
    "            line_split = line.split(\"\\t\")\n",
    "            \n",
    "            # make sure the split produces the expected number of arguments\n",
    "            assert(len(line_split) == 2),\\\n",
    "            \"> expected 2 tab separated values, found {0} instead\".format(len(line_split))\n",
    "            \n",
    "            # take the token from the line\n",
    "            test_token = line_split[0]\n",
    "            \n",
    "            # take the concept from the line\n",
    "            test_concept = line_split[1]\n",
    "            \n",
    "            # insert the token in the list of tokens per sentence in the test file \n",
    "            list_of_test_tokens.append(test_token)\n",
    "            \n",
    "            # insert the concept in the list of concepts per sentence in the test file \n",
    "            list_of_test_concepts.append(test_concept)\n",
    "    \n",
    "    # close the tagged file\n",
    "    result_tagged_file.close()\n",
    "    \n",
    "    # close the test file\n",
    "    test_file.close()\n",
    "    \n",
    "    # return the tagged file path\n",
    "    return result_tagged_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function evaluates the result of the tagging process\n",
    "def evaluate_file(result_tagged_file_path, output_dir):\n",
    "    \n",
    "    print(\"> evaluating file ...\")\n",
    "    \n",
    "    # define the evaluation file path\n",
    "    result_evaluation_file_path = \"{0}/evaluation.txt\".format(output_dir)\n",
    "    \n",
    "    # call the evaluation script\n",
    "    assert(subprocess.call(\"./conlleval.pl < {0} > {1}\"\\\n",
    "                           .format(result_tagged_file_path, result_evaluation_file_path), shell = True) == 0)\n",
    "    \n",
    "    # return the evaluation file path\n",
    "    return result_evaluation_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "> unable to mkdir /AF_none_IM_naive_SM_kneser_ney_NS_4_HU_uniform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-9708e67906c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# make another directory with the specified output directory name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mkdir -p {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"> unable to mkdir {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# copy the config file to the output directoryt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: > unable to mkdir /AF_none_IM_naive_SM_kneser_ney_NS_4_HU_uniform"
     ]
    }
   ],
   "source": [
    "# remove the output directory if already exists\n",
    "assert(subprocess.call(\"rm -rf {0}\".format(OUTPUT_DIR), shell = True) == 0),\\\n",
    "\"> unable to remove {0}\".format(OUTPUT_DIR)\n",
    "\n",
    "# make another directory with the specified output directory name\n",
    "assert(subprocess.call(\"mkdir -p {0}\".format(OUTPUT_DIR), shell = True) == 0),\\\n",
    "\"> unable to mkdir {0}\".format(OUTPUT_DIR)\n",
    "\n",
    "# copy the config file to the output directoryt\n",
    "assert(subprocess.call(\"cp {0} {1}\".format(CONFIGURATION_FILE, OUTPUT_DIR), shell = True) == 0),\\\n",
    "\"unable to copy {0} to {1}\".format(CONFIGURATION_FILE, OUTPUT_DIR)\n",
    "\n",
    "# define the train temporary file path\n",
    "train_temp_file_path = \"{0}/{1}\".format(OUTPUT_DIR, \"train_temp.txt\")\n",
    "\n",
    "# define the test temporary file path\n",
    "test_temp_file_path = \"{0}/{1}\".format(OUTPUT_DIR, \"test_temp.txt\")\n",
    "\n",
    "# copy the train file to the output directory and rename it\n",
    "assert(subprocess.call(\"cp {0} {1}\".format(TRAIN_FILE, train_temp_file_path), shell = True) == 0),\\\n",
    "\"unable to copy {0} to {1}\".format(TRAIN_FILE, train_temp_file_path)\n",
    "\n",
    "# copy the test file to the output directory and rename it\n",
    "assert(subprocess.call(\"cp {0} {1}\".format(TEST_FILE, test_temp_file_path), shell = True) == 0),\\\n",
    "\"unable to copy {0} to {1}\".format(TEST_FILE, test_temp_file_path)\n",
    "\n",
    "# check if additional feature is requested\n",
    "if ADDITIONAL_FEATURE != \"none\":\n",
    "    \n",
    "    # apply additional feature to the train file\n",
    "    train_additional_feature_file_path = apply_additional_feature(train_temp_file_path, TRAIN_FEATS_FILE, ADDITIONAL_FEATURE)\n",
    "    \n",
    "    # apply additional feature to the test file\n",
    "    test_additional_feature_file_path = apply_additional_feature(test_temp_file_path, TEST_FEATS_FILE, ADDITIONAL_FEATURE)\n",
    "    \n",
    "    # check if improvement is requested\n",
    "    if IMPROVEMENT == \"wise\":\n",
    "        \n",
    "        # apply improvement to the train file\n",
    "        train_additional_feature_wise_file_path = apply_wise_improvement(train_additional_feature_file_path)\n",
    "        \n",
    "        # apply improvement to the test file\n",
    "        test_additional_feature_wise_file_path = apply_wise_improvement(test_additional_feature_file_path)\n",
    "        \n",
    "        # generate lexicon and transducer\n",
    "        lexicon_file_path, transducer_file_path = generate_lexicon_and_transducer(train_additional_feature_wise_file_path, HANDLE_UNK, OUTPUT_DIR)\n",
    "        \n",
    "        # compile transducer\n",
    "        transducer_fst_file_path = compile_transducer(lexicon_file_path, transducer_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate concept sentences file\n",
    "        concept_sentences_file_path = create_concept_sentences(train_additional_feature_wise_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate language model\n",
    "        language_model_lm_file_path = create_language_model(lexicon_file_path, concept_sentences_file_path, NGRAM_SIZE, SMOOTHING, BACKOFF, BINS, WITTEN_BELL_K, DISCOUNT_D, OUTPUT_DIR)\n",
    "        \n",
    "        # tag the test file\n",
    "        result_tagged_file_path = tag_file(test_additional_feature_wise_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, OUTPUT_DIR)       \n",
    "        \n",
    "        # evaluate the results\n",
    "        evaluation_file_path = evaluate_file(result_tagged_file_path, OUTPUT_DIR)\n",
    "    \n",
    "    # check if improvement is requested\n",
    "    elif IMPROVEMENT == \"naive\":\n",
    "        \n",
    "        # apply improvement to the train file\n",
    "        train_additional_feature_naive_file_path = apply_naive_improvement(train_additional_feature_file_path)\n",
    "        \n",
    "        # apply improvement to the test file\n",
    "        test_additional_feature_naive_file_path = apply_naive_improvement(test_additional_feature_file_path)\n",
    "        \n",
    "        # generate lexicon and transducer\n",
    "        lexicon_file_path, transducer_file_path = generate_lexicon_and_transducer(train_additional_feature_naive_file_path, HANDLE_UNK, OUTPUT_DIR)\n",
    "        \n",
    "        # compile transducer\n",
    "        transducer_fst_file_path = compile_transducer(lexicon_file_path, transducer_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate concept sentences file\n",
    "        concept_sentences_file_path = create_concept_sentences(train_additional_feature_naive_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate language model\n",
    "        language_model_lm_file_path = create_language_model(lexicon_file_path, concept_sentences_file_path, NGRAM_SIZE, SMOOTHING, BACKOFF, BINS, WITTEN_BELL_K, DISCOUNT_D, OUTPUT_DIR)\n",
    "        \n",
    "        # tag the test file\n",
    "        result_tagged_file_path = tag_file(test_additional_feature_naive_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, OUTPUT_DIR)       \n",
    "        \n",
    "        # evaluate the results\n",
    "        evaluation_file_path = evaluate_file(result_tagged_file_path, OUTPUT_DIR)\n",
    "    \n",
    "    else: # IMPROVEMENT == \"none\"\n",
    "        \n",
    "        # generate lexicon and transducer\n",
    "        lexicon_file_path, transducer_file_path = generate_lexicon_and_transducer(train_additional_feature_file_path, HANDLE_UNK, OUTPUT_DIR)\n",
    "        \n",
    "        # compile transducer\n",
    "        transducer_fst_file_path = compile_transducer(lexicon_file_path, transducer_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate concept sentences file\n",
    "        concept_sentences_file_path = create_concept_sentences(train_additional_feature_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate language model\n",
    "        language_model_lm_file_path = create_language_model(lexicon_file_path, concept_sentences_file_path, NGRAM_SIZE, SMOOTHING, BACKOFF, BINS, WITTEN_BELL_K, DISCOUNT_D, OUTPUT_DIR)\n",
    "        \n",
    "        # tag the test file\n",
    "        result_tagged_file_path = tag_file(test_additional_feature_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, OUTPUT_DIR)       \n",
    "        \n",
    "        # evaluate the results\n",
    "        evaluation_file_path = evaluate_file(result_tagged_file_path, OUTPUT_DIR)\n",
    "\n",
    "else: # ADDITIONAL_FEATURE == \"none\"\n",
    "    \n",
    "    # check if improvement is requested\n",
    "    if IMPROVEMENT == \"wise\":\n",
    "        \n",
    "        # apply improvement to the train file\n",
    "        train_wise_file_path = apply_wise_improvement(train_temp_file_path)\n",
    "        \n",
    "        # apply improvement to the test file\n",
    "        test_wise_file_path = apply_wise_improvement(test_temp_file_path)\n",
    "        \n",
    "        # generate lexicon and transducer\n",
    "        lexicon_file_path, transducer_file_path = generate_lexicon_and_transducer(train_wise_file_path, HANDLE_UNK, OUTPUT_DIR)\n",
    "        \n",
    "        # compile transducer\n",
    "        transducer_fst_file_path = compile_transducer(lexicon_file_path, transducer_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate concept sentences file\n",
    "        concept_sentences_file_path = create_concept_sentences(train_wise_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate language model\n",
    "        language_model_lm_file_path = create_language_model(lexicon_file_path, concept_sentences_file_path, NGRAM_SIZE, SMOOTHING, BACKOFF, BINS, WITTEN_BELL_K, DISCOUNT_D, OUTPUT_DIR)\n",
    "        \n",
    "        # tag the test file\n",
    "        result_tagged_file_path = tag_file(test_wise_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, OUTPUT_DIR)       \n",
    "        \n",
    "        # evaluate the results\n",
    "        evaluation_file_path = evaluate_file(result_tagged_file_path, OUTPUT_DIR)\n",
    "    \n",
    "    # check if improvement is requested\n",
    "    elif IMPROVEMENT == \"naive\":\n",
    "        \n",
    "        # apply improvement to the train file\n",
    "        train_naive_file_path = apply_naive_improvement(train_temp_file_path)\n",
    "        \n",
    "        # apply improvement to the test file\n",
    "        test_naive_file_path = apply_naive_improvement(test_temp_file_path)\n",
    "        \n",
    "        # generate lexicon and transducer\n",
    "        lexicon_file_path, transducer_file_path = generate_lexicon_and_transducer(train_naive_file_path, HANDLE_UNK, OUTPUT_DIR)\n",
    "        \n",
    "        # compile transducer\n",
    "        transducer_fst_file_path = compile_transducer(lexicon_file_path, transducer_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate concept sentences file\n",
    "        concept_sentences_file_path = create_concept_sentences(train_naive_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate language model\n",
    "        language_model_lm_file_path = create_language_model(lexicon_file_path, concept_sentences_file_path, NGRAM_SIZE, SMOOTHING, BACKOFF, BINS, WITTEN_BELL_K, DISCOUNT_D, OUTPUT_DIR)\n",
    "        \n",
    "        # tag the test file\n",
    "        result_tagged_file_path = tag_file(test_naive_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, OUTPUT_DIR)       \n",
    "        \n",
    "        # evaluate the results\n",
    "        evaluation_file_path = evaluate_file(result_tagged_file_path, OUTPUT_DIR)\n",
    "    \n",
    "    else: # IMPROVEMENT == \"none\"\n",
    "        \n",
    "        # generate lexicon and transducer\n",
    "        lexicon_file_path, transducer_file_path = generate_lexicon_and_transducer(train_temp_file_path, HANDLE_UNK, OUTPUT_DIR)\n",
    "        \n",
    "        # compile transducer\n",
    "        transducer_fst_file_path = compile_transducer(lexicon_file_path, transducer_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate concept sentences file\n",
    "        concept_sentences_file_path = create_concept_sentences(train_temp_file_path, OUTPUT_DIR)\n",
    "        \n",
    "        # generate language model\n",
    "        language_model_lm_file_path = create_language_model(lexicon_file_path, concept_sentences_file_path, NGRAM_SIZE, SMOOTHING, BACKOFF, BINS, WITTEN_BELL_K, DISCOUNT_D, OUTPUT_DIR)\n",
    "        \n",
    "        # tag the test file\n",
    "        result_tagged_file_path = tag_file(test_temp_file_path, lexicon_file_path, transducer_fst_file_path, language_model_lm_file_path, OUTPUT_DIR)       \n",
    "        \n",
    "        # evaluate the results\n",
    "        evaluation_file_path = evaluate_file(result_tagged_file_path, OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
